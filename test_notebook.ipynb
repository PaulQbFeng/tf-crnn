{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/notebooks/models/research/slim/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "try:\n",
    "    import better_exceptions\n",
    "except ImportError:\n",
    "    pass\n",
    "import tensorflow as tf\n",
    "from src.model import crnn_fn\n",
    "from src.data_handler import make_input_fn\n",
    "from src.data_handler import preprocess_image_for_prediction\n",
    "\n",
    "from src.config import Params, import_params_from_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_cnn(input_imgs: tf.Tensor, is_training=False, summaries=False):\n",
    "\n",
    "    input_tensor = input_imgs\n",
    "    if input_tensor.shape[-1] == 1:\n",
    "        input_channels = 1\n",
    "    elif input_tensor.shape[-1] == 3:\n",
    "        input_channels = 3\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # Following source code, not paper\n",
    "\n",
    "    with tf.variable_scope('deep_cnn'):\n",
    "        # - conv1 - maxPool2x2\n",
    "        with tf.variable_scope('layer1'):\n",
    "            W = weightVar([3, 3, input_channels, 64])\n",
    "            b = biasVar([64])\n",
    "            conv = conv2d(input_tensor, W, name='conv')\n",
    "            out = tf.nn.bias_add(conv, b)\n",
    "            conv1 = tf.nn.relu(out)\n",
    "            pool1 = tf.nn.max_pool(conv1, [1, 2, 2, 1], strides=[1, 2, 2, 1],\n",
    "                                   padding='SAME', name='pool')\n",
    "\n",
    "            if summaries:\n",
    "                weights = [var for var in tf.global_variables() if var.name == 'deep_cnn/layer1/weights:0'][0]\n",
    "                tf.summary.histogram('weights', weights)\n",
    "                bias = [var for var in tf.global_variables() if var.name == 'deep_cnn/layer1/bias:0'][0]\n",
    "                tf.summary.histogram('bias', bias)\n",
    "\n",
    "\n",
    "        # - conv2 - maxPool 2x2\n",
    "        with tf.variable_scope('layer2'):\n",
    "            W = weightVar([3, 3, 64, 128])\n",
    "            b = biasVar([128])\n",
    "            conv = conv2d(pool1, W)\n",
    "            out = tf.nn.bias_add(conv, b)\n",
    "            conv2 = tf.nn.relu(out)\n",
    "            pool2 = tf.nn.max_pool(conv2, [1, 2, 2, 1], strides=[1, 2, 2, 1],\n",
    "                                   padding='SAME', name='pool1')\n",
    "\n",
    "            if summaries:\n",
    "                weights = [var for var in tf.global_variables() if var.name == 'deep_cnn/layer2/weights:0'][0]\n",
    "                tf.summary.histogram('weights', weights)\n",
    "                bias = [var for var in tf.global_variables() if var.name == 'deep_cnn/layer2/bias:0'][0]\n",
    "                tf.summary.histogram('bias', bias)\n",
    "\n",
    "        # - conv3 - w/batch-norm (as source code, not paper)\n",
    "        with tf.variable_scope('layer3'):\n",
    "            W = weightVar([3, 3, 128, 256])\n",
    "            b = biasVar([256])\n",
    "            conv = conv2d(pool2, W)\n",
    "            out = tf.nn.bias_add(conv, b)\n",
    "            b_norm = tf.layers.batch_normalization(out, axis=-1,\n",
    "                                                   training=is_training, name='batch-norm')\n",
    "            conv3 = tf.nn.relu(b_norm, name='ReLU')\n",
    "\n",
    "            if summaries:\n",
    "                weights = [var for var in tf.global_variables() if var.name == 'deep_cnn/layer3/weights:0'][0]\n",
    "                tf.summary.histogram('weights', weights)\n",
    "                bias = [var for var in tf.global_variables() if var.name == 'deep_cnn/layer3/bias:0'][0]\n",
    "                tf.summary.histogram('bias', bias)\n",
    "\n",
    "        # - conv4 - maxPool 2x1\n",
    "        with tf.variable_scope('layer4'):\n",
    "            W = weightVar([3, 3, 256, 256])\n",
    "            b = biasVar([256])\n",
    "            conv = conv2d(conv3, W)\n",
    "            out = tf.nn.bias_add(conv, b)\n",
    "            conv4 = tf.nn.relu(out)\n",
    "            pool4 = tf.nn.max_pool(conv4, [1, 2, 2, 1], strides=[1, 2, 1, 1],\n",
    "                                   padding='SAME', name='pool4')\n",
    "\n",
    "            if summaries:\n",
    "                weights = [var for var in tf.global_variables() if var.name == 'deep_cnn/layer4/weights:0'][0]\n",
    "                tf.summary.histogram('weights', weights)\n",
    "                bias = [var for var in tf.global_variables() if var.name == 'deep_cnn/layer4/bias:0'][0]\n",
    "                tf.summary.histogram('bias', bias)\n",
    "\n",
    "        # - conv5 - w/batch-norm\n",
    "        with tf.variable_scope('layer5'):\n",
    "            W = weightVar([3, 3, 256, 512])\n",
    "            b = biasVar([512])\n",
    "            conv = conv2d(pool4, W)\n",
    "            out = tf.nn.bias_add(conv, b)\n",
    "            b_norm = tf.layers.batch_normalization(out, axis=-1,\n",
    "                                                   training=is_training, name='batch-norm')\n",
    "            conv5 = tf.nn.relu(b_norm)\n",
    "\n",
    "            if summaries:\n",
    "                weights = [var for var in tf.global_variables() if var.name == 'deep_cnn/layer5/weights:0'][0]\n",
    "                tf.summary.histogram('weights', weights)\n",
    "                bias = [var for var in tf.global_variables() if var.name == 'deep_cnn/layer5/bias:0'][0]\n",
    "                tf.summary.histogram('bias', bias)\n",
    "\n",
    "        # - conv6 - maxPool 2x1 (as source code, not paper)\n",
    "        with tf.variable_scope('layer6'):\n",
    "            W = weightVar([3, 3, 512, 512])\n",
    "            b = biasVar([512])\n",
    "            conv = conv2d(conv5, W)\n",
    "            out = tf.nn.bias_add(conv, b)\n",
    "            conv6 = tf.nn.relu(out)\n",
    "            pool6 = tf.nn.max_pool(conv6, [1, 2, 2, 1], strides=[1, 2, 1, 1],\n",
    "                                   padding='SAME', name='pool6')\n",
    "\n",
    "            if summaries:\n",
    "                weights = [var for var in tf.global_variables() if var.name == 'deep_cnn/layer6/weights:0'][0]\n",
    "                tf.summary.histogram('weights', weights)\n",
    "                bias = [var for var in tf.global_variables() if var.name == 'deep_cnn/layer6/bias:0'][0]\n",
    "                tf.summary.histogram('bias', bias)\n",
    "\n",
    "        # - conv 7 - w/batch-norm (as source code, not paper)\n",
    "        with tf.variable_scope('layer7'):\n",
    "            W = weightVar([2, 2, 512, 512])\n",
    "            b = biasVar([512])\n",
    "            conv = conv2d(pool6, W, padding='VALID')\n",
    "            out = tf.nn.bias_add(conv, b)\n",
    "            b_norm = tf.layers.batch_normalization(out, axis=-1,\n",
    "                                                   training=is_training, name='batch-norm')\n",
    "            conv7 = tf.nn.relu(b_norm)\n",
    "\n",
    "            if summaries:\n",
    "                weights = [var for var in tf.global_variables() if var.name == 'deep_cnn/layer7/weights:0'][0]\n",
    "                tf.summary.histogram('weights', weights)\n",
    "                bias = [var for var in tf.global_variables() if var.name == 'deep_cnn/layer7/bias:0'][0]\n",
    "                tf.summary.histogram('bias', bias)\n",
    "\n",
    "        cnn_net = conv7\n",
    "\n",
    "        with tf.variable_scope('Reshaping_cnn'):\n",
    "            shape = cnn_net.get_shape().as_list()  # [batch, height, width, features]\n",
    "            transposed = tf.transpose(cnn_net, perm=[0, 2, 1, 3],\n",
    "                                      name='transposed')  # [batch, width, height, features]\n",
    "            conv_reshaped = tf.reshape(transposed, [shape[0], -1, shape[1] * shape[3]],\n",
    "                                       name='reshaped')  # [batch, width, height x features]\n",
    "\n",
    "    return conv_reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp /notebooks/samples/0.png /tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import io, transform\n",
    "\n",
    "def resize_img(img):\n",
    "    return transform.rescale(img, 512 / img.shape[1], mode='constant', cval=255, clip=True, preserve_range=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 256)\n",
      "(1, 32, 256, 1)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAABMCAYAAAB9PUwnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4xLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvDW2N/gAACTpJREFUeJzt3V3IXEcdx/Hvz6cvoi1o0hCeptG0Ui96oWkItWApQtG0uYnelCpohEJuLFhQMNqbXlbBgoIIkRZaKUahlfaiEmtRxAvTJCVNm4akT2tKE9PEqtiC0Nr69+Kcx2ye7nn2bfa8zPl94OHZPbs7Z2bOnP/OmZndVURgZmbd94GmM2BmZmk4oJuZZcIB3cwsEw7oZmaZcEA3M8uEA7qZWSZmCuiSbpV0XNKSpN2pMmVmZpPTtOvQJS0AJ4DPA6eAA8CXI+LFdNkzM7NxzdJDvwFYiohXIuIdYC+wI022zMxsUhfN8NoNwGsD908Bn1ntBVesWYhNGy8G4MSRD73v8U9+6t8zZIfk6Q5La9Y0q9LuStlT5HNY+inzOY+67FOaw9JO2d7bmseV6c0r3WnSO3Tk7TciYt2o580S0MciaRewC+BjGy7imX0bAdh25eYLnrfvr4eBtTPvbx7p1pHXffsOz5ze+9JMWPYiLWZOb2XaKdM9n166PJ7X5zRTp9uFPM4z3cnTW1hcenWc580y5HIa2Dhw/6py2wUiYk9EbI2IrevWLsywOzMzW80sk6IXUUyK3kIRyA8AX4mIo1Wv2frpD8ZyD93MzMazsLh0KCK2jnre1EMuEfGupLuAfcAC8OBqwdzMzOZrpjH0iHgSeDJRXszMbAb+pKiZWSYc0M3MMjH3ZYtmZnVbudR42fllsnlyQDezLFQF8WHPyTWwe8jFzCwTDuhmZplwQDezzlttzDzX4ZVhHNDNrLO2Xbl5qgnQccbbu8gB3cw6abWgPKpXnmuv3QHdzCwTXrZoZp3T13Xmozigm1lnzDLM0gcO6FMYNaHihmVWr3HPudzPTY+hm5llwj30MU2yzOnCn1YzsxSGnYOjzrNclydWcUBfxaiJl741lpx5kq3dpgnmfTT1T9BNoys/QTdp43Ew6KZx35B9HJs1bTAf/uPu3TTuT9B5DN3MLBMjh1wkbQQeBtYDAeyJiB9JWgP8EtgEnARuj4h/zi+r9Rh8V59k5tzDL+3nY9R9Xe5l12GcMfR3gW9FxLOSLgcOSXoK+DrwdETcJ2k3sBv4zvyyOj8pxueGBfUuTY7mOmw0ThDv6htyl9rXNGY5Jl08nimMDOgRcQY4U95+S9IxYAOwA/hc+bSHgD/QwYCecpytq5Olq+W3i0FjmrHxwdttfGOuKtNq7XfYY9NcgTYh9SRom8u6rOpYFtuXxkpjojF0SZuA64H9wPoy2AO8TjEkM+w1uyQdlHTwb39/b5LdmZnZBMZetijpMuBR4O6IeFPS/x+LiJA0dLlMROwB9kCxymW27KZT1wx423/yqmtXE6Pk9ineSY/PqKutlffbWB99XKI4rMzTnJtjBXRJF1ME80ci4rFy81lJixFxRtIicG7ivTegj41lmNwCOaQL5lXzIZOkkUIdx6hNQT3FPE7XliqmPsbjrHIR8ABwLCLuH3joCWAncF/5//GkOUvIX+hzodyC+Tx65VXzIXUFwDqPURuuIvvW0ZrX8R2nh/5Z4KvA85KWa/h7FIH8V5LuBF4Fbp9LDmeUY0OZZGJr1obT9rrKbYgF0pzsk0zQt7GOpslTF3rn836jHmeVy58AVTx8S9rsmJnZtLL7Lpcce2yDqsZ2p9XVpZZQz7Gu8/MFkxyDWb4utm1LF7vQs57FtOfWYD0sLI73miwCelsuK7sUFFdbr9wFfZwXSVWuttRPynbXtjeFacqWIs+dDuhtCeRdsrI+cv2EaG76cDxSjJt3Ucpj6y/nMjPLRCd76F0aJ0+dl1E97BQf725T/a3UpWM/ib5dKaXoWbdxBVvTX8ncuYDex7HT1Qwr8zTfFT3Ja5vSVDBv6oNGbT4WTWvbUEvTgXxZ5wL6MG1o+G1rYKtpY89mlJzfyLt4PGYx6ovGqsrexquzts3jeQzdzCwTneqht21pUhd16UoC2tkrS6WLw14pjPr++ba30bb1ygfV+puikt4Cjte2w/a6Anij6Uw0zHVQcD24DmB0HXw8ItaNSqTuHvrxcX7oNHeSDva9HlwHBdeD6wDS1YHH0M3MMuGAbmaWiboD+p6a99dWrgfXwTLXg+sAEtVBrZOiZmY2Px5yMTPLhAO6mVkmagvokm6VdFzSkqTdde23aZJOSnpe0mFJB8ttayQ9Jeml8v9Hm85napIelHRO0gsD24aWW4Ufl23jiKQtzeU8nYo6uFfS6bI9HJa0feCx75Z1cFzStmZynZakjZJ+L+lFSUclfbPc3re2UFUPadtDRMz9D1gAXgauAS4BngOuq2PfTf8BJ4ErVmz7AbC7vL0b+H7T+ZxDuW8GtgAvjCo3sB34DcVPHd4I7G86/3Osg3uBbw957nXleXEpcHV5viw0XYYEdbAIbClvXw6cKMvat7ZQVQ9J20NdPfQbgKWIeCUi3gH2Ajtq2ncb7QAeKm8/BHyxwbzMRUT8EfjHis1V5d4BPByFPwMfkTTmj261V0UdVNkB7I2ItyPiL8ASxXnTaRFxJiKeLW+/BRwDNtC/tlBVD1Wmag91BfQNwGsD90+xemFyEsBvJR2StKvctj4izpS3XwfWN5O12lWVu2/t465yOOHBgeG27OtA0ibgemA/PW4LK+oBErYHT4rO300RsQW4DfiGpJsHH4zi+qp3a0f7Wm7gp8AngM3AGeCHzWanHpIuAx4F7o6INwcf61NbGFIPSdtDXQH9NLBx4P5V5bbsRcTp8v854NcUl01nly8jy//nmsthrarK3Zv2ERFnI+K9iPgv8DPOX0ZnWweSLqYIYo9ExGPl5t61hWH1kLo91BXQDwDXSrpa0iXAHcATNe27MZI+LOny5dvAF4AXKMq+s3zaTuDxZnJYu6pyPwF8rVzhcCPwr4HL8aysGA/+EkV7gKIO7pB0qaSrgWuBZ+rOX2qSBDwAHIuI+wce6lVbqKqH5O2hxlne7RQzuy8D9zQ961xTma+hmKl+Dji6XG5gLfA08BLwO2BN03mdQ9l/QXEJ+R+K8b87q8pNsaLhJ2XbeB7Y2nT+51gHPy/LeKQ8aRcHnn9PWQfHgduazn+iOriJYjjlCHC4/Nvew7ZQVQ9J24M/+m9mlglPipqZZcIB3cwsEw7oZmaZcEA3M8uEA7qZWSYc0M3MMuGAbmaWif8BSaA0WKtxb4EAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = cv2.imread('/tmp/0.png', 0)[:32,:256]\n",
    "plt.imshow(img)\n",
    "\n",
    "print(img.shape)\n",
    "# img = resize_img(img).reshape(1, -1, 512, 1)\n",
    "img = img.reshape(1,32,-1,1).astype(np.float32)\n",
    "\n",
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weightVar(shape, mean=0.0, stddev=0.02, name='weights'):\n",
    "    init_w = tf.truncated_normal(shape=shape, mean=mean, stddev=stddev)\n",
    "    return tf.Variable(init_w, name=name)\n",
    "\n",
    "\n",
    "def biasVar(shape, value=0.0, name='bias'):\n",
    "    init_b = tf.constant(value=value, shape=shape)\n",
    "    return tf.Variable(init_b, name=name)\n",
    "\n",
    "\n",
    "def conv2d(input, filter, strides=[1, 1, 1, 1], padding='SAME', name=None):\n",
    "    return tf.nn.conv2d(input, filter, strides=strides, padding=padding, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Graph().as_default():\n",
    "    conv = deep_cnn(img.astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'conv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-bf36e666a8d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mconv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'conv' is not defined"
     ]
    }
   ],
   "source": [
    "conv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resnet-50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nets.resnet_v1 import resnet_v1_50, resnet_v1_101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_cnn(cnn_input: tf.Tensor):\n",
    "    with tf.variable_scope('Reshaping_cnn'):\n",
    "        shape = cnn_input.get_shape().as_list()  # [batch, height, width, features]\n",
    "        transposed = tf.transpose(cnn_input, perm=[0, 2, 1, 3],\n",
    "                                  name='transposed')  # [batch, width, height, features]\n",
    "        conv_reshaped = tf.reshape(transposed, [shape[0], -1, shape[1] * shape[3]],\n",
    "                                   name='reshaped')  # [batch, width, height x features]\n",
    "    return conv_reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"resnet_v1_50/block4/unit_3/bottleneck_v1/Relu:0\", shape=(1, 1, 8, 2048), dtype=float32)\n",
      "Tensor(\"Reshaping_cnn/reshaped:0\", shape=(1, 8, 2048), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    conv_res,_ = resnet_v1_50(img, global_pool=False, on_text=False)\n",
    "    print(conv_res)\n",
    "    conv_res = reshape_cnn(conv_res)\n",
    "    print(conv_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resnet-101 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"resnet_v1_101/block4/unit_3/bottleneck_v1/Relu:0\", shape=(1, 1, 8, 2048), dtype=float32)\n",
      "Tensor(\"Reshaping_cnn/reshaped:0\", shape=(1, 8, 2048), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    conv_res,_ = resnet_v1_101(img, global_pool=False, on_text=False)\n",
    "    print(conv_res)\n",
    "    conv_res = reshape_cnn(conv_res)\n",
    "    print(conv_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### on_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"resnet_v1_101/block4/unit_3/bottleneck_v1/Relu:0\", shape=(1, 1, 63, 2048), dtype=float32)\n",
      "Tensor(\"Reshaping_cnn/reshaped:0\", shape=(1, 63, 2048), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    conv_res_on_text,_ = resnet_v1_101(img, global_pool=False, on_text=True)\n",
    "    print(conv_res_on_text)\n",
    "    conv_res_on_text = reshape_cnn(conv_res_on_text)\n",
    "    print(conv_res_on_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check graph ckpt pretrained resnet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /notebooks/resnet_pretrained/model.ckpt.data-00000-of-00001\n"
     ]
    },
    {
     "ename": "DataLossError",
     "evalue": "Unable to open table file /notebooks/resnet_pretrained/model.ckpt.data-00000-of-00001: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator?\n\t [[node save/RestoreV2 (defined at <ipython-input-30-d712e1b2f646>:3)  = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\n\nCaused by op 'save/RestoreV2', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.5/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/platform/asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/lib/python3.5/asyncio/base_events.py\", line 345, in run_forever\n    self._run_once()\n  File \"/usr/lib/python3.5/asyncio/base_events.py\", line 1312, in _run_once\n    handle._run()\n  File \"/usr/lib/python3.5/asyncio/events.py\", line 125, in _run\n    self._callback(*self._args)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/ioloop.py\", line 758, in _run_callback\n    ret = callback()\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/gen.py\", line 1233, in inner\n    self.run()\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 357, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 267, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 534, in execute_request\n    user_expressions, allow_stdin,\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2819, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2845, in _run_cell\n    return runner(coro)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 3020, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 3185, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 3267, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-30-d712e1b2f646>\", line 3, in <module>\n    saver = tf.train.import_meta_graph('/notebooks/resnet_pretrained/model.ckpt.meta')\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py\", line 1674, in import_meta_graph\n    meta_graph_or_file, clear_devices, import_scope, **kwargs)[0]\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py\", line 1696, in _import_meta_graph_with_return_elements\n    **kwargs))\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/meta_graph.py\", line 806, in import_scoped_meta_graph_with_return_elements\n    return_elements=return_elements)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/importer.py\", line 442, in import_graph_def\n    _ProcessNewOps(graph)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/importer.py\", line 234, in _ProcessNewOps\n    for new_op in graph._add_new_tf_operations(compute_devices=False):  # pylint: disable=protected-access\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 3440, in _add_new_tf_operations\n    for c_op in c_api_util.new_tf_operations(self)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 3440, in <listcomp>\n    for c_op in c_api_util.new_tf_operations(self)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 3299, in _create_op_from_tf_operation\n    ret = Operation(c_op, self)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1770, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nDataLossError (see above for traceback): Unable to open table file /notebooks/resnet_pretrained/model.ckpt.data-00000-of-00001: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator?\n\t [[node save/RestoreV2 (defined at <ipython-input-30-d712e1b2f646>:3)  = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDataLossError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDataLossError\u001b[0m: Unable to open table file /notebooks/resnet_pretrained/model.ckpt.data-00000-of-00001: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator?\n\t [[{{node save/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mDataLossError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-d712e1b2f646>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0msess\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_meta_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/notebooks/resnet_pretrained/model.ckpt.meta'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'/notebooks/resnet_pretrained/model.ckpt.data-00000-of-00001'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1544\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1545\u001b[0m         sess.run(self.saver_def.restore_op_name,\n\u001b[0;32m-> 1546\u001b[0;31m                  {self.saver_def.filename_tensor_name: save_path})\n\u001b[0m\u001b[1;32m   1547\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNotFoundError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1548\u001b[0m       \u001b[0;31m# There are three common conditions that might cause this error:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1346\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDataLossError\u001b[0m: Unable to open table file /notebooks/resnet_pretrained/model.ckpt.data-00000-of-00001: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator?\n\t [[node save/RestoreV2 (defined at <ipython-input-30-d712e1b2f646>:3)  = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\n\nCaused by op 'save/RestoreV2', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.5/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/platform/asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/lib/python3.5/asyncio/base_events.py\", line 345, in run_forever\n    self._run_once()\n  File \"/usr/lib/python3.5/asyncio/base_events.py\", line 1312, in _run_once\n    handle._run()\n  File \"/usr/lib/python3.5/asyncio/events.py\", line 125, in _run\n    self._callback(*self._args)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/ioloop.py\", line 758, in _run_callback\n    ret = callback()\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/gen.py\", line 1233, in inner\n    self.run()\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 357, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 267, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 534, in execute_request\n    user_expressions, allow_stdin,\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2819, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2845, in _run_cell\n    return runner(coro)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 3020, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 3185, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 3267, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-30-d712e1b2f646>\", line 3, in <module>\n    saver = tf.train.import_meta_graph('/notebooks/resnet_pretrained/model.ckpt.meta')\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py\", line 1674, in import_meta_graph\n    meta_graph_or_file, clear_devices, import_scope, **kwargs)[0]\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py\", line 1696, in _import_meta_graph_with_return_elements\n    **kwargs))\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/meta_graph.py\", line 806, in import_scoped_meta_graph_with_return_elements\n    return_elements=return_elements)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/importer.py\", line 442, in import_graph_def\n    _ProcessNewOps(graph)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/importer.py\", line 234, in _ProcessNewOps\n    for new_op in graph._add_new_tf_operations(compute_devices=False):  # pylint: disable=protected-access\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 3440, in _add_new_tf_operations\n    for c_op in c_api_util.new_tf_operations(self)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 3440, in <listcomp>\n    for c_op in c_api_util.new_tf_operations(self)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 3299, in _create_op_from_tf_operation\n    ret = Operation(c_op, self)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1770, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nDataLossError (see above for traceback): Unable to open table file /notebooks/resnet_pretrained/model.ckpt.data-00000-of-00001: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator?\n\t [[node save/RestoreV2 (defined at <ipython-input-30-d712e1b2f646>:3)  = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    sess= tf.Session()\n",
    "    saver = tf.train.import_meta_graph('/notebooks/resnet_pretrained/model.ckpt.meta')\n",
    "    saver.restore(sess, '/notebooks/resnet_pretrained/model.ckpt.data-00000-of-00001')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
